{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d6769f63486c9058f5e02c7220ce410da7feb2b73f9089aecd1314889f470a2b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## ML Pipeline Preparation\n",
    "\n",
    "### Main blocks of code\n",
    "* Importing libraries and loading data from database\n",
    "* Writing a tokenization function to process text data\n",
    "* Building a machine learning Pipeline\n",
    "* Improving model with grid search and testing new model\n",
    "* Export model as a pickle file\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "______________________________"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importing libraries and load data from database"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "#engine = create_engine('sqlite:///disaster_responses.db', echo=True)\n",
    "# table named disaster_responses will be returned as a dataframe\n",
    "#df = pd.read_sql_table('disaster_responses', con=engine)\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\BernadettKepenyes\\Documents\\GitHub\\disaster-response-pipelines\\data\\disaster_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract message column\n",
    "X = df['message']\n",
    "\n",
    "# classification labels\n",
    "# Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1), or:\n",
    "y = df.iloc[:, 4:]"
   ]
  },
  {
   "source": [
    "_____________________"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Writing a tokenization function to process text data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization function to process text data\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    function: returning the root form of the words of messages\n",
    "    input: message text(str)\n",
    "    output: cleaned list of words of messages\n",
    "    '''\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # normalizing text\n",
    "    words = word_tokenize(text) # tokenizing text\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # removing stop words\n",
    "    lemmatizer = WordNetLemmatizer() # initiating text\n",
    "    \n",
    "    # lemmatizing - iterate through each token\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        clean = lemmatizer.lemmatize(w)\n",
    "        clean_words.append(clean)\n",
    "    \n",
    "    return clean_words\n",
    "\n",
    "# testing out function\n",
    "for message in X[:5]:\n",
    "    words = tokenize(message)\n",
    "    print(message)\n",
    "    print(words, '\\n')"
   ]
  },
  {
   "source": [
    "________________"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building a machine learning pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "\n",
    "# splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# fit Random Forest Classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "# prediction\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing function\n",
    "def model_report(y_test, y_pred):\n",
    "    i = 0\n",
    "    for col in y_test:\n",
    "        print('Category {}: {}'.format(i+1, col))\n",
    "        print(classification_report(y_test[col], y_pred[:, i]))\n",
    "        i = i+1\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    print('Accuracy: ', accuracy)\n",
    "\n",
    "model_report(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "_________________"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Improving model with grid search and testing new model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "parameters = {\n",
    "            'clf__estimator__n_estimators': [60]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model\n",
    "Y_pred = model.predict(X_test)\n",
    "    \n",
    "i = 0\n",
    "for col in Y_test:\n",
    "    print('Category {}: {}'.format(i+1, col))\n",
    "    print(classification_report(Y_test[col], Y_pred[:, i]))\n",
    "    i = i+1\n",
    "accuracy = (Y_pred == Y_test).mean()\n",
    "print('Accuracy: ', accuracy)\n",
    "sample_accuracy = accuracy.mean()\n",
    "print('Average accuracy: ', sample_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "pickle.dump(model, open(r\"C:\\Users\\BernadettKepenyes\\Documents\\GitHub\\disaster-response-project\\models\", 'wb'))"
   ]
  }
 ]
}